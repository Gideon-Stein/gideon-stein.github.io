@article{asadulaev2019jacobian,
  title={Jacobian Policy Optimizations.},
  author={Asadulaev, Arip and Stein, Gideon and Kuznetsov, Igor and Filchenkov, Andrey},
  journal={CoRR},
  year={2019}
}

@article{stein2020stabilizing,
  title={Stabilizing Transformer-Based Action Sequence Generation For Q-Learning},
  author={Stein, Gideon and Filchenkov, Andrey and Asadulaev, Arip},
  journal={arXiv preprint arXiv:2010.12698},
  preview=stein2020stabilizing.png,
  year={2020}
}

@article{asadulaev2019conditioning,
  title={Conditioning of Reinforcement Learning Agents and its Policy Regularization Application},
  author={Asadulaev, Arip and Kuznetsov, Igor and Stein, Gideon and Filchenkov, Andrey},
  journal={arXiv preprint arXiv:1906.05437},
  year={2019},
  preview=asadulaev2019conditioning.png

}

@article{asadulaev2020exploring,
  title={Exploring and exploiting conditioning of reinforcement learning agents},
  author={Asadulaev, Arip and Kuznetsov, Igor and Stein, Gideon and Filchenkov, Andrey},
  journal={IEEE Access},
  volume={8},
  pages={211951--211960},
  year={2020},
  preview=asadulaev2020exploring.png,
  publisher={IEEE}
}

@inproceedings{festag2022outcome,
  title={Outcome Prediction and Murmur Detection in Sets of Phonocardiograms by a Deep Learning-Based Ensemble Approach},
  author={Festag, Sven and Stein, Gideon and B{\"u}chner, Tim and Shadaydeh, Maha and Denzler, Joachim and Spreckelsen, Cord},
  booktitle={2022 Computing in Cardiology (CinC)},
  volume={498},
  pages={1--4},
  year={2022},
  preview=festag2022outcome.png,
  organization={IEEE}
}

@article{huang2023plant,
  title={Plant diversity stabilizes soil temperature},
  author={Huang, Yuanyuan and Stein, Gideon and Kolle, Olaf and K{\"u}bler, Karl and Schulze, Ernst-Detlef and Dong, Hui and Eichenberg, David and Gleixner, Gerd and Hildebrandt, Anke and Lange, Markus and others},
  journal={bioRxiv},
  pages={2023--03},
  year={2023},
  preview=huang2023plant.png,
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{dubois2022ki4ki,
  title={KI4KI: Neues Projekt zur regelm{\"a}{\ss}igen {\"U}berwachung von Stauanlagen aus dem All},
  author={Dubois, Clemence and J{\"a}nichen, Jannik and Shadaydeh, Maha and Stein, Gideon and Katz, Alexandra and Kl{\"o}pper, Daniel and Denzler, Joachim and Schmullius, Christiane and Last, Katja},
  booktitle={Messtechnische {\"U}berwachung von Stauanlagen; XII. Mittweidaer Talsperrentag},
  number={1},
  pages={15--19},
  year={2022},
  organization={Hochschule Mittweida}
}

@article{huang2024enhanced,
  title={Enhanced stability of grassland soil temperature by plant diversity},
  author={Huang, Yuanyuan and Stein, Gideon and Kolle, Olaf and K{\"u}bler, Karl and Schulze, Ernst-Detlef and Dong, Hui and Eichenberg, David and Gleixner, Gerd and Hildebrandt, Anke and Lange, Markus and others},
  journal={Nature Geoscience},
  volume={17},
  number={1},
  pages={44--50},
  year={2024},
  preview=huang202enhanced.png,
  publisher={Nature Publishing Group UK London}
}

@article{stein2024embracing,
  title={Embracing the black box: Heading towards foundation models for causal discovery from time series data},
  author={Stein, Gideon and Shadaydeh, Maha and Denzler, Joachim},
  journal={arXiv preprint arXiv:2402.09305},
  preview=stein2024embracing.png,
  year={2024}
}

@article{penzel2024reducing,
  title={Reducing Bias in Pre-trained Models by Tuning while Penalizing Change},
  author={Penzel, Niklas and Stein, Gideon and Denzler, Joachim},
  journal={arXiv preprint arXiv:2404.12292},
  preview=penzel2024reducing.png,
  year={2024}
}

@article{piater2024medical,
  title={When Medical Imaging Met Self-Attention: A Love Story That Didn't Quite Work Out},
  author={Piater, Tristan and Penzel, Niklas and Stein, Gideon and Denzler, Joachim},
  journal={arXiv preprint arXiv:2404.12295},
  preview=piater2024medical.png,
  year={2024}
}

@inproceedings{stein2024investigating,
  title={Investigating the effects of plant diversity on soil thermal diffusivity using Physics-Informed Neural Networks},
  author={Stein, Gideon and Vemuri, Sai Karthikeya and Huang, Yuanyuan and Ebeling, Anne and Eisenhauer, Nico and Shadaydeh, Maha and Denzler, Joachim},
  booktitle={ICLR 2024 Workshop on AI4DifferentialEquations In Science},
  preview=stein2024investigating.png,
  year={2024}
}

@inproceedings{stein2024data,
  title={Data-Driven Prediction of Large Infrastructure Movements Through Persistent Scatterer Time Series Modeling},
  author={Stein, Gideon and Ziemer, Jonas and Wicker, Carolin and J{\"a}nichen, Jannik and Demisch, Gabriele and Kl{\"o}pper, Daniel and Last, Katja and Denzler, Joachim and Schmullius, Christiane and Shadaydeh, Maha and others},
  booktitle={IGARSS 2024-2024 IEEE International Geoscience and Remote Sensing Symposium},
  pages={8669--8673},
  year={2024},
  preview=stein2024datadriven.png,
  organization={IEEE}
}

@article{bonato2025seasonal,
  title={Seasonal shifts in plant diversity effects on above-ground--below-ground phenological synchrony},
  author={Bonato Asato, Ana E and Guimar{\~a}es-Steinicke, Claudia and Stein, Gideon and Schreck, Berit and Kattenborn, Teja and Ebeling, Anne and Posch, Stefan and Denzler, Joachim and B{\"u}chner, Tim and Shadaydeh, Maha and others},
  journal={Journal of Ecology},
  preview=asato2025seasonal.png,
  year={2025}
}



@article{piater2024selfattention,
    type = {article},
    key = {piater2024selfattention},
    title = {Self-Attention for Medical Imaging - On the need for evaluations beyond mere benchmarking},
    author = {Tristan Piater and Niklas Penzel and Gideon Stein and Joachim Denzler},
    journal = {Communications in Computer and Information Science},
    year = {2025},
    month = {},
    pages = {},
    doi = {},
    url = {},
    preview=piater2024selfattention.png,
    publisher = {Springer International Publishing},
    abstract = {A considerable amount of research has been dedicated to creating systems that aid medical professionals in labor-intensive early screening tasks, which, to this date, often leverage convolutional deep-learning architectures. Recently, several studies have explored the application of self-attention mechanisms in the field of computer vision. These studies frequently demonstrate empirical improvements over traditional, fully convolutional approaches across a range of datasets and tasks. To assess this trend for medical imaging, we enhance two commonly used convolutional architectures with various self-attention mechanisms and evaluate them on two distinct medical datasets.  We compare these enhanced architectures with similarly sized convolutional and attention-based baselines and rigorously assess performance gains through statistical evaluation. Furthermore, we investigate how the inclusion of self-attention influences the features learned by these models by assessing global and local explanations of model behavior. Contrary to our expectations, after performing an appropriate hyperparameter search, self-attention-enhanced architectures show no significant improvements in balanced accuracy compared to the evaluated baselines. Further, we find that relevant global features like dermoscopic structures in skin lesion images are not properly learned by any architecture.  Finally, by assessing local explanations, we find that the inherent interpretability of self-attention mechanisms does not provide additional insights. Out-of-the-box model-agnostic approaches can provide explanations that are similar or even more faithful to the actual model behavior.  We conclude that simply integrating attention mechanisms is unlikely to lead to a consistent increase in performance compared to fully convolutional methods in medical imaging applications.},
    note = {(in press)},
}

@article{penzel2024change,
    type = {article},
    key = {penzel2024change},
    title = {Change Penalized Tuning to Reduce Pre-trained Biases},
    author = {Niklas Penzel and Gideon Stein and Joachim Denzler},
    journal = {Communications in Computer and Information Science},
    year = {2025},
    month = {},
    pages = {},
    doi = {},
    url = {},
    preview=penzel2024change.png,
    publisher = {Springer International Publishing},
    abstract = {Due to the data-centric approach of modern machine learning, biases present in the training data are frequently learned by deep models. It is often necessary to collect new data and retrain the models from scratch to remedy these issues, which can be expensive in critical areas such as medicine. We investigate whether it is possible to fix pre-trained model behavior using very few unbiased examples. We show that we can improve performance by tuning the models while penalizing parameter changes. Hence, we are keeping pre-trained knowledge while simultaneously correcting the harmful behavior. Toward this goal, we tune a zero-initialized copy of the frozen pre-trained network using strong parameter norms. Secondly, we introduce an early stopping scheme to modify baselines and reduce overfitting. Our approaches lead to improvements in four datasets common in the debiasing and domain shift literature. We especially see benefits in an iterative setting, where new samples are added continuously. Hence, we demonstrate the effectiveness of tuning while penalizing change to fix pre-trained models without retraining from scratch.},
    note = {(in press)},
}

@inproceedings{stein2025causalrivers,
    type = {inproceedings},
    key = {stein2025causalrivers},
    title = {CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series},
    author = {Gideon Stein and Maha Shadaydeh and Jan Blunk and Niklas Penzel and Joachim Denzler},
    booktitle = {The Thirteenth International Conference on Learning Representations},
    year = {2025},
    url = {https://openreview.net/forum?id=wmV4cIbgl6},
    langid = {english},
    preview=ci.png,
    note = {(accepted at ICLR 2025)},
}
